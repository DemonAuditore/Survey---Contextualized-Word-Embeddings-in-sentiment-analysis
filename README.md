# Survey---Contextualized-Word-Embeddings-in-sentiment-analysis

## pre-training models
### Transformer
Transformer, the entire network structure of which is completely composed of attention mechanisms, more precisely, the self-attention and the Feed Forward Neural Network.

The main motivations of it are parallel computation and feature extraction problems in traditional RNN and CNN models *[paper 1]*. As shown in *[figure 1]*, RNN models and Transformer perform much better than CNN models in terms of long-range dependencies. However, *[figure 2]* shows that due to differences in model structure (Mainly the use of attention mechanisms in Transformer), CNN and Transformer has significant advantages over RNN in reducing computation time with parallel computing *[paper 2]*.

These characteristics of the transformer can be mainly attributed to the use of Multi-head self-attention mechanisms. Also, more interpretable models could be generated by self-attention, which may could benefit fields like medical treatment *[paper 3]*.

 - #### Model architecture
   - ##### Encoder-decoder frame
      The general frame of encoder-decoder structure is shown in figure 1. 
   - ##### General architecture
     - ##### Encoder
     - ##### Decoder
 - #### Attention
 - #### FFN
 - #### Positional encoding

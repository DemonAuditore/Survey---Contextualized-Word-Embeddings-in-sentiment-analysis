# Survey---Contextualized-Word-Embeddings-in-sentiment-analysis

## pre-training models
### Transformer
Transformer, the entire network structure of which is completely composed of attention mechanisms, more precisely, the self-attention and the Feed Forward Neural Network.

The main motivations of it are parallel computation and feature extraction problems in traditional RNN and CNN models *[paper 1]*. As shown in *[figure 1]*, RNN models and Transformer perform much better than CNN models in terms of long-range dependencies. However, *[figure 2]* shows that due to differences in model structure (Mainly the use of attention mechanisms in Transformer), CNN and Transformer has significant advantages over RNN in reducing computation time with parallel computing *[paper 2]*.

These characteristics of the transformer can be mainly attributed to the use of Multi-head self-attention mechanisms. Also, more interpretable models could be generated by self-attention, which may could benefit fields like medical treatment *[paper 3]*.

- #### Model architecture
   In essence, the transformer is an Encoder-decoder model that can be represented as the structure of *[figure 3]*:
   <p align="center">
   <img src="https://cdn-images-1.medium.com/max/1600/1*Ismhi-muID5ooWf3ZIQFFg.png" width="700">
   <br><i>Figure3: Encoder-decoder architecture</i>
   </p>
   The encoder reads the input and generate a vector called context vector while the decoder yields next word given c and all the previously predicted words *[paper 4]*. 
   <p align="center">
   <img src="https://cdn-images-1.medium.com/max/1600/1*BHzGVskWGS_3jEcYYi6miQ.png" width="700">
   <br><i>Figure4: Transformer</i>
   </p>
    For Transformer, encoding component is a stack of 6 encoder blocks, each block is composed of two sublayers:  multi-head attention and feed forward network. The decoder has both layers in encoder, but inserts a masked multi-head attention layer before them. Also, residual connection and layer normalization are empolyed for all mentioned layers. The overall architecture is shown in *[figure 4]*.

 - #### Attention
 - #### FFN
 - #### Positional encoding
